## Predictive Modeling

Based on some of the above correlations and supporting evidence from the US Forest Service Research Archives [2], there are strong correlation between diameter at breast heights (dbh) and tree age, tree height, leaf area, crown height, and average crown diameter. 
Therefore, several predictive models using 3 different machine learning techniques were explored to select the most suitable model to predict (dbh) based on the aforementioned variables. Those techniques include (1) decision-tree algorithm, (2) regression, and (3) neural networks. To enable a reliable performance evaluation procedure, the collected data was divided into two separate datasets for each developed predictive model: (1) training dataset that includes 70% of all the available data that will be used in developing the model, and (2) testing dataset that includes 30% of all available data that will be used for evaluating the performance of the developed model. The following three sections provide a detailed description of these aforementioned 3 machine learning techniques.

### Decision-Tree Algorithm 

This model was run with two to five independent variables in order to predict DBH. The output of these iterations are as follows:


![Decision Tree using 2 independent variables: Tree Height and Age](images/DT_AgeHt.png){#fig:DT_AgeHt width=3in}

![Decision Tree using 3 independent variables: Tree Height, Age, and Leaf Area](images/DT_AgeHtLeaf.png){#fig:DT_AgeHtLeaf width=3in}

![Decision Tree using 4 independent variabless: Tree Height, Age, Leaf Area, and Crown Height](images/DT_ageHtLeafCrnHt.png){#fig:DT_AgeHtLeafCrnHt width=3in}

![Decision Tree using 5 independent variabless: Tree Height, Age, Leaf Area, Crown Height, and Average Crown Diameter](images/DT_ageHtLeafCrnHtCDia.png){#fig:DT_AgeHtLeafCrnHtCdia width=3in}

With each addition of independent variables, the mean coefficient of determination increased, and the mean squared error decreased. Overall the decision tree model for this data is not robust and does not do a great job at fitting the data. The correlation coefficient gets increasingly closer to one with additional variables, meaning the linear relation is better with more variables and DBH than a few variables and DBH. This finding shows that a linear regression model might be a better method for modeling this dataset.

A classification decision tree was also tested to see if the tree type could be predicted using crown height, age, DBH, and tree height. This model was run with three folds similarly to the previous decision tree. The resulting model had very low accuracy of 0.369198, 0.377247, and 0.376442. These values of accuracy suggest that predicting tree type in this manner is not reliable.

### Regression

This technique depends on developing multiple linear regression model between the dependent variable and independent variables. To improve the accuracy of the model, average dbh was calculated to each tree age. Starting with a simple regression model that include only one independent variable (tree age) to predict the dependent variable (average dbh). The predictive formula is used to calculate the coefficient of determination, Root Mean Square Error (RMSE), and model accuracy. As shown in Figure @fig:Reg_model1, the model achieved R squared of 74%, Root Mean Square Error (RMSE) of 21.12, and accuracy of 12.4%. 

![Predictive Model using one independent variable](images/Reg_Model1.png){#fig:Reg_model1 width=6in}

Further investigation was performed to determine why the accuracy is low. After plotting average dbh in y-axis and age in the x-axis for training and testing datasets. Two data points were determined to be outliers, as shown in Figure @fig:Reg_model1_plot

![Determing outliars](images\Reg_model1_plot.png){#fig:Reg_model1_plot width=5in}

The second model was performed using the same dependent and independent variables after excluding outliersâ€™ data point. Those two outliers were determined to be tree age that were above 200 years.  The model achieved R squared of 88%, RMSE of 10.91, and accuracy of 87%, as shown in Figure @fig:Reg_Model2

![Predictive Model after Deleting Outliers](images\Reg_Model2.png){#fig:Reg_Model2 width=6in}

The third model was performed using two independent variables: tree age and average tree height. The model achieved R squared of 94%, RMSE of 8.08, and accuracy of 92%, as shown in Figure @fig:Reg_Model3

![Predictive Model Using 2 Independent Variables](images\Reg_Model3.png){#fig:Reg_Model3 width=6in}

The fourth model was performed using three independent variables: tree age, average tree height, and average leaf area. The model achieved R squared of 95%, RMSE of 9.01, and accuracy of 92%, as shown in Figure @fig:Reg_Model4

![Predictive Model Using 3 Independent Variables](images\Reg_Model4.png){#fig:Reg_Model4 width=6in}

The fifth model was performed using four independent variables: tree age, average tree height, average leaf area, and average crown diameter. The model achieved R squared of 93%, RMSE of 7, and accuracy of 94.5%, as shown in Figure @fig:Reg_Model5

![Predictive Model Using 4 Independent Variables](images\Reg_Model5.png){#fig:Reg_Model5 width=6in}

Based on the above analysis, the best model that achieved the lowest RMSE and highest accuracy is Model 5.


### Neural Network
This final technique involves two main approaches. The first builds a simple linear regression neural network of one tree characteristic input to one tree characteristic output. The aim of this approach was to get a simple neural network running on tree data. First, the tree data was filtered into "DBH", "TreeHt", "Age", and "CrnBase." Then, "DBH" was chosen as the input variable and "TreeHt" was selected as the output or dependent variable for prediction. The observations in the dataframe were then shuffled to prepare to split the dataset into 50% training and 50% testing data. Following the splitting and reshaping of the data, a simple linear regression neural network was constructed in the Julia Flux packaging using one dense layer with one input and one output channel, and the choice of optimization was gradient descent while the loss function was based on mean square error (MSE). After running through 12 epochs or iterations, the neural net predicted 88% of tree heights from given DBH values. The RSME associated with this model was roughly 13.8. This accuracy is good, but not excellent or sufficient to validate actual predictions. 

Moving on to a more complex neural network, the second technique tackled more input or independent variables to predict tree species. After isolating relevant tree characteristic input features such as "TreeType", "Age", "DBH", "TreeHt", "CrnBase", "CrnHt", "CdiaPar", "CDiaPerp", "AvgCdia", and "Leaf" and filtering out unwanted missing data, the neural net was structured to predict tree species, or "CommonName." 157 unique tree species names were identified, and these were manipulated to create unique integer indeces mapped to each unique "CommonName." Following appropriate reshaping and data re-structuring to meet the required input format in the Flux Package, a convolutional neural network (CNN) with 5 convolutional layers of varying filter size and 2 dense layers was built and run over 125 epochs with ADAM as the optimization algorithm and cross entropy as the loss function. ADAM, as opposed to stochastic gradient descent, is able to incorporate concepts of momentum rather than randomness to push the gradient descent algorithm out of local minima and isolate the global minima. The result of this CNN produced a very low accuracy of 4%.

Due to this low accuracy, a more simplified NN on tree species was performed with only 2 dense layers and no convolutional filters. With the same optimization and loss functions that were used in the previous CNN, the result of this neural net also produced a low prediction accuracy of 4.5%. Though this increased slightly, the poor accuracy presents a larger concern regarding the strategy of data prediction. Because only 9 input features were used to predict 157 unique tree species, it is more likely that the result of this low accuracy is not the model itself but the sheer variety of tree species in comparison to the variety of tree characteristics like height or DBH. 

Because of this, a NN was constructed to predict tree type (which has 11 unique tree types) instead of tree species to simplify the outputs being predicted. To briefly recall, tree types are 3 letter codes, where the first two letters refer to life form (BD=broadleaf deciduous, BE=broadleaf evergreen, CE=coniferous evergreen, PE=palm evergreen) and the third letter refers to the tree's mature height (S=small, which is < 8 meters, M=medium, which is 8-15 meters, and L=large, which is > 15 meters). Starting simply with only 2 dense layers, the neural network yielded a better but still poor accuracy of 41%.

To try improving this, a CNN was performed on tree type with 5 convolutional layers of increasing and decreasing filter sizes and 2 dense layers. Although CNNs are typically used for problems involving spatial patterns, we tried building one anyway to see if prediction accuracy could be improved. A slightly better accuracy of 47% was in fact achieved, which could suggest how more complex convolutional layers might prove more effective. Moving forward, accuracy of this model will be re-structured for better improvement.

The following image shows the accuracy of the CNN using 5 convolutional layers and 2 dense layers to predict tree type over many iterations.

![Convolutional Neural Network Predicting TreeType](images/E_CNN_TreeType.png){#fig:E_CNN_TreeType width=6in}


### Summary of Model Comparison
To summarize all models that were performed, the table below shows the inputs, outputs, and associated accuracy for each model in terms of R^2 and RSME.